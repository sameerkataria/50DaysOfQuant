{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Linear regression using Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is a first order methord for solving an optimization problem\n",
    "- Gradient Descent with a step size $t_k$ at the $k^{th}$ iteration\n",
    "    - $ x ^ {k+1} = x^{k} - t_k \\nabla f(x^k)$\n",
    "    - Here $\\nabla f(x^k)$ is the gradient at point $x^k$\n",
    "- This method is commonly used in Deep learning and machine learning to minimize cost functions\n",
    "- For convex optimization it gives global optimum under failry general conditions\n",
    "- for non-convex optimization, it may achieve a local optimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> In relation to Linear regression we are trying to minimize the squared errors </b>\n",
    "- $min f(b) = || y-Xb ||^{2}$\n",
    "    - $X \\in R^{n\\times(p+1)}$\n",
    "- $\\nabla f(b) = 2X^{T}(Xb-y)$\n",
    "- $b^{k+1} = b^{k} - 2t_{k} X^{T}(Xb^{k}-y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Some of the stopping criterion could be the following </b>\n",
    "- $||\\nabla f(x)||_{2}$ is sufficiently small\n",
    "- $||x^{k+1}-x^{k}||_{2}$ or $|f(x^{k+1})-f(x^{k})|$ is small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Learning Rate in a gradient Descent</b>\n",
    "<br>![Alt Text](https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/national/gradient-descent-learning-rate.png)</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Link for More on hyperparameters](https://developers.google.com/machine-learning/crash-course/linear-regression/hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some types of Gradient Descent\n",
    "- Batch Gradient Descent\n",
    "- Stochastic Gradient Descent\n",
    "- Mini batch Gradient Descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Link to diff between batch and SGD](https://www.geeksforgeeks.org/difference-between-batch-gradient-descent-and-stochastic-gradient-descent/#:~:text=background%2Dcolor%3A%20%23ffffff%3B%20%7D-,Batch%20Gradient%20Descent,can%20help%20reduce%20overfitting%20by%20updating%20the%20model%20parameters%20more%20frequently.,-Comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
